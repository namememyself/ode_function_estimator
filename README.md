Neural Networks for Differential Equations: PINNs & Classical Approaches
This repository contains two complementary approaches for solving differential equations using neural networks: Physics-Informed Neural Networks (PINNs) with advanced features and Classical Neural Network ODE Solvers.

ğŸ”¬ Overview
This project demonstrates cutting-edge techniques for solving ordinary and partial differential equations using deep learning, featuring both foundational methods and state-of-the-art implementations with hyperparameter optimization and attention mechanisms.

ğŸ“ Repository Structure
text
â”œâ”€â”€ pinns_Advanced.ipynb          # Advanced PINN framework with modern ML techniques
â”œâ”€â”€ NN_ODE_RC_Notebook-1.ipynb    # Classical neural network ODE solver
â”œâ”€â”€ README.md                     # This file
â””â”€â”€ requirements.txt              # Dependencies
ğŸš€ Quick Start
Prerequisites
bash
pip install torch torchvision numpy matplotlib scipy optuna tqdm
Running the Notebooks
Advanced PINNs: Open pinns_Advanced.ipynb for sophisticated PINN implementations

Classical Methods: Open NN_ODE_RC_Notebook-1.ipynb for foundational neural ODE solving

ğŸ“Š Features Comparison
Feature	Advanced PINNs	Classical NN ODE
Hyperparameter Optimization	âœ… Optuna integration	âŒ Manual tuning
Attention Mechanisms	âœ… Feature-wise attention	âŒ Standard architecture
Uncertainty Quantification	âœ… Ensemble methods	âŒ Single model
Boundary Conditions	âœ… Flexible (Dirichlet/Neumann)	âœ… Trial function approach
User-Defined ODEs	âœ… Lambda function support	âœ… Hardcoded examples
Computational Efficiency	âš¡ Optimized with tuning	âš¡ Fast and simple
Learning Curve	ğŸ”´ Advanced	ğŸŸ¢ Beginner-friendly
ğŸ§  Advanced PINN Framework
Key Innovations
Hyperparameter Autotuning with Optuna

Automated optimization of learning rates, network architecture, and loss weights

50+ trials for finding optimal configurations

Best achieved loss: ~0.0019

Attention Mechanism Integration

Feature-wise attention gates for enhanced learning

Selective feature weighting during training

Improved convergence and solution quality

Ensemble Uncertainty Quantification

Multiple model training with different random seeds

Statistical analysis of prediction variance

Confidence bounds for solution reliability

Flexible Architecture

python
# Example configuration
main(perform_tuning=True, use_attention=True)
Core Components
PINN Architecture
python
class PINN(nn.Module):
    def __init__(self, layers, activation=nn.Tanh, use_attention=False):
        # Customizable depth, width, and attention integration
        # Support for Tanh, ReLU, and other activation functions
Physics Loss Implementation
python
def physics_loss(model, x, ode_func):
    # Automatic differentiation for derivative computation
    # Residual calculation for differential equation enforcement
Hyperparameter Optimization
python
def objective(trial):
    # Optuna-based parameter space exploration
    # Multi-objective optimization for physics and boundary losses
ğŸ”§ Classical Neural Network ODE Solver
Lagaris Method Implementation
Based on the foundational approach by Lagaris et al., this implementation provides:

Trial Function Construction

Automatic boundary condition satisfaction

Custom function design for specific problems

Neural Network Architecture

python
class Fitter(nn.Module):
    # Simple feedforward network
    # Configurable hidden layers and nodes
Applications

General ODE solving

RC circuit analysis

Educational examples with analytical comparisons

Example Problems
General ODE: dy/dx = f(x, y) with initial conditions

RC Circuit: First-order differential equation for circuit analysis

ğŸ“ˆ Performance Results
Advanced PINNs
Optimized Loss: 0.00227 (best configuration)

Training Epochs: 3000 (full training)

Hyperparameter Trials: 50 with Optuna

Ensemble Size: 5 models for uncertainty

Classical NN ODE
Final Training Cost: 0.053

MSE vs Analytical: 0.005 (RC circuit)

Training Time: 2.77 seconds (100 epochs)

Architecture: 10 hidden nodes

ğŸ¯ Use Cases & Applications
When to Use Advanced PINNs
âœ… Complex geometries or boundary conditions

âœ… Limited experimental data

âœ… Need for uncertainty quantification

âœ… Research and development

âœ… Multi-physics problems

When to Use Classical NN ODE
âœ… Simple 1D problems

âœ… Educational purposes

âœ… Fast prototyping

âœ… Well-defined boundary conditions

âœ… Computational efficiency requirements

ğŸ› ï¸ Technical Implementation Details
Automatic Differentiation
Both approaches leverage PyTorch's automatic differentiation for computing derivatives required in differential equations.

Loss Functions
Physics Loss: Enforces differential equation residuals

Boundary Loss: Satisfies initial/boundary conditions

Combined Loss: Weighted sum with tunable parameters

Optimization Strategies
Adam Optimizer: Adaptive learning rates

Learning Rate Scheduling: Dynamic adjustment during training

Ensemble Training: Multiple models for robustness

ğŸ“š Mathematical Foundation
PINNs Formulation
text
Total Loss = Î»â‚ Ã— Physics_Loss + Î»â‚‚ Ã— BC_Loss
Physics_Loss = ||âˆ‚u/âˆ‚t + N[u]||Â²
BC_Loss = ||u(boundary) - g||Â²
Classical Approach
text
Trial Function: Ïˆ(x) = A + x Ã— N(x)
Loss = ||dÏˆ/dx - f(x,Ïˆ)||Â²
ğŸ”„ Workflow
Problem Definition: Define ODE and boundary conditions

Model Configuration: Choose architecture and parameters

Training: Optimize physics and boundary losses

Evaluation: Compare with analytical solutions

Uncertainty Analysis: Ensemble predictions (PINNs only)

ğŸ¤ Contributing
Contributions are welcome! Please feel free to submit pull requests for:

New differential equation examples

Additional attention mechanisms

Improved optimization strategies

Documentation enhancements

ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ™ Acknowledgments
Physics-Informed Neural Networks methodology by Raissi et al.

Lagaris method for neural network ODE solving

Optuna framework for hyperparameter optimization

PyTorch team for automatic differentiation capabilities

ğŸ“– References
Raissi, M., Perdikaris, P., & Karniadakis, G. E. (2019). Physics-informed neural networks

Lagaris, I. E., Likas, A., & Fotiadis, D. I. (1998). Artificial neural networks for solving ordinary and partial differential equations

Optuna: A Next-generation Hyperparameter Optimization Framework

Get Started: Clone this repository and explore both pinns_Advanced.ipynb for cutting-edge techniques and NN_ODE_RC_Notebook-1.ipynb for foundational understanding!
